---
title: Attention-based sequence prediction 이해하기
layout: single
author_profile: true
read_time: true
use_math: true
comments: true
share: true
related: true
categories:
- str
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: STR
article_tag2: scene text recognition
article_tag3: FAN
article_tag4: focusing attention network
last_modified_at: 2020-04-28 18:07:00 +0800
---

Attention-based sequence prediction에 관한 글입니다.

[참고 논문]
- [[RARE]](https://arxiv.org/abs/1603.03915) Robust Scene Text Recognition with Automatic Rectification _ 1603
- [[FAN]](https://arxiv.org/abs/1709.02054) Focusing attention: Towards accurate text recognition in natural images _ 1709
- [[AON]](https://arxiv.org/abs/1711.04226) AON: Towards Arbitrarily-Oriented Text Recognition _ 1711
- [[EP]](https://arxiv.org/abs/1805.03384) Edit Probability for Scene Text Recognition _ 1805


## RARE

TPS-VGG-BiLSTM-Attn

### SRN : Sequence Recognition Network

- attention-based model로, input image로부터 sequence를 인식한다.
- input : rectified image(STN output), 이상적으로 문자들이 왼쪽에서 오른쪽으로 수평적 이미지
- input으로부터 sequential representation을 extract하고, 단어를 인식한다.
- SRN은 encoder와 decoder를 가진다.
    - encoder : input image로부터 sequential representation을 extract한다.
    - decoder : sequential representation에 따른 sequence를 생성한다.

![Figure 5](/assets/images/post/attn/figure5_attn.PNG)

#### 1. Encoder : Convolutional-Recurrent Network

CNN으로부터 feature map을 추출하여, map-to-sequence 연산을 통해 feature map을 여러 연속된 조각으로 분리한다.

[Figure 5]
- several convolutional layers : (input image의 robust하고 high-level의 descriptions이 포함된) feature maps 생성한다.
    - feature maps : depth D * height H * width W
- map-to-sequence operation : map의 columns를 왼쪽에서 오른쪽으로 가져와 vectors로 flatten한다.
    - sequence of W vectors : DW dimensions
    - column : local image region(receptive field)에 대응하고, 해당 region에 대한 descriptor이다.
- BLSTM : 2 layer Bidirectional Long-Short Term Memory network
    - receptive field 크기에 의해 제한되므로, feature sequence는 limited image contexts에 영향을 준다.?
    - sequence 내의 long-term dependencies를 model하기 위해서 사용한다.
    - 양방향으로 sequence 내의 dependencies를 분석할 수 있는 recurrent network
    - output은 input과 동일한 길이(L = W)의 sequence


#### 2. Decoder : Recurrent Character Generator



## FAN : Focusing Attention Network

?-ResNet-?-Attn

**기존 attention-based method 문제**
- complicated, low-quality 이미지에서 성능이 떨어짐
- cannot get accurate alignments between feature areas and targets for such images
--> **"attention drift"**

**논문에서 구현한 scene text recognition**
1. ResNet 기반 CNN을 이용해 deeper representation을 extract한다. (아마 이 논문이 처음)
2. alignment factors와 glimpse vectors를 생성하기 위해 sequence of features를 AN으로 보낸다.
3. 이때, FN을 이용해 glimpse vectors가 reasonable한지 판단하고, AN이 보다 reasonable한 glimpse vectors를 생성할 수 있도록 feedback을 제공한다. 이를 통해, AN은 처리된 이미지에서 target characters의 올바른 영역에 적절하게 attention할 수 있다.

### FAN method

FAN = AN + FN

Figure2 (a)를 보면 attention이 제대로 이루어지지 않고 있다. 이러한 문제를 "attention drift"라고 한다. (b)와 같이 FN을 추가하여 문제점을 해결한다.

![Figure 2](/assets/images/post/attn/figure2.png)

- alignment factors(target labels & features 사이) 생성된다. 각 alighment factor는 input image의 attention region과 대응한다.
- bad alignment(벗어나거나 unfocused attention region)는 poor recognition을 보인다.
- FN component는...
    - 1\) 각 target label에 대해 attention region 위치를 찾고,
    - 2\) 대응하는 glimpse vector와 함께 attention region으로부터 dense prediction을 한다.
- 이렇게 FN은 glimpse vector가 reasonable한지 판단할 수 있다.

![Figure 5](/assets/images/post/attn/figure5_fan.PNG)

summary
- FN은 AN에서 제공하는 glimpse vector를 바탕으로, input image의 attention region에 대해 dense output을 생성한다.
- AN은 FN의 feedback을 바탕으로, glimpse vector를 업데이트한다.

#### AN : Attention Network

recognizing character targets (기존 방법처럼)

##### 1. attention based decoder
- RNN, inut image I로부터 target sequence를 직접 생성한다.
    - 실제로, image $$I$$는 CNN-LSTM에 의해 a sequence of feature vectors로 종종 encode된다.
    $$I = Encoder(h_1, ... , h_T)$$

![Formula 1](/assets/images/post/attn/formula1.png){: width="60%" height="60%"}
- $$g_t$$ : glimpse vector
- Generate() : feed-forward network
- RNN() : LSTM recurrent network

- EOS(end-of-sentence) token을 target set에 추가한다.
- decoder는 가변적인 길이의 sequence를 다루기 때문에 EOS가 나오면 문자 생성을 완료한다.

##### 2. Loss Function

![Formula 6](/assets/images/post/attn/formula6.png){: width="60%" height="60%"}

##### 3. drawbacks

1\) attention drift
- complicated, low-quality 이미지에 영향을 많이 받는다.
- 정확하지 않은 alignment factors를 생성한다.
    - integration of glimpse vectors에 대한 alignment constraint(제약)이 모델에 없기 때문이다.
    - 이로 인해, attention regions과 ground-truth regions가 불일치 할 수 있다.

- FAN 논문에서는 이를 해결하는 것을 목표로 한다.
- [[FN]](#fn-:-focusing-network)을 도입하여, 각 target character로 AN의 attention을 제한하고자 한다.

2\) huge scene text data(e.g. 8000만 개 synthetic data)에 대해서는 모델을 train하기 어렵다.


#### FN : Focusing Network

adjusting attention by evaluating whether AN pays attention properly on the target areas in the images

- attention model에서, 각 feature vector는 input image의 영역과 mapping 된다.
- 이는 convolution strategy를 바탕으로 target character를 localize하는데 사용될 수 있다.
- BUT! 계산된 targets의 attention regions 일반적으로 정확하지 않다. 특히, complicated & low-quality 이미지에서.
- 이러한 attention drift 문제를 바로잡기 위해 FN을 소개한다.

![Figure 3](/assets/images/post/attn/figure3.png)



##### 1. computing attention center
: computing the attention center of each predicted label

convolution/pooling operation
- input : $$N * D_i * H_i * W_i$$
- output : $$N * D_o * H_o * W_o$$<br>
(N : batch size, D : number of channels, H/W : height/width of feature maps)

layer L의  (x,y)에 대해, (7)과 같이 bounding box coordinates r로 layer L-1의 receptive field을 계산한다.

![Formula 7](/assets/images/post/attn/formula7.png){: width="60%" height="60%"}

t번째 step에서
(7)을 반복적으로 계산함으로써 input image에서의 $$h_j$$의 receptive field을 계산하고, attention center인 recetive field의 center를 선택한다.

$$c_{t,j} = location(j) \quad (8)$$
- location() : receptive field의 center를 계산하는 함수.

input image의 target $$y_t$$의 attention center는 (9)와 같이 계산한다.

$$c_t =  \sum_{j=1}^T \alpha_{t,j} c_{t,j} \quad (9)$$


##### 2. focusing attention on target regions
: focusing attention on target regions by generating the probability distributions on the attention regions

위에서 계산한 target $$y_t$$의 attention center를 이용하여, input image 또는 convolution layer output에서 사이즈가 $$P(P_H, P_W)$$인 feature maps의 patch를 자른다.

$$F_t = Crop(F, c_t, P_H, P_W \quad (10)$$

- $$F$$ : image 또는 convolution feature maps
- $$P$$ : input image에서 ground-truth regions의 최대 크기

cropped feature maps를 이용하여, attention region에 대한 energy distribution을 계산한다.

$$e_t^{(i,j)} = tanh(R_{g_t} + S F_t^{(i,j)} + b) \quad (11)$$

- $$R, S$$ : trainable parameters
- (i,j) : $$(i * P_W + j)$$번째 feature vector를 가리킨다.
- glimpse vector와 attention region의 $$(i,j)$$번째 feature 값의 energy 크기를 나타낸다.

선택한 region(잘라낸 patch)에 대한 probability distribution(확률 분포)은 다음과 같이 계산한다.

![Formula 12](/assets/images/post/attn/formula12.png){: width="60%" height="60%"}
- $$K$$ : label class 개수

focussing loss function

![Formula 13](/assets/images/post/attn/formula13.png){: width="60%" height="60%"}
- $$y_t^{(i,j)}$$ : ground-truth pixel label
- $$w$$ : 모든 FN parameters를 결합한 vector
- loss는 character annotations를 가진 image의 subset에 대해서만 추가된다.(character label gt가 존재하는 경우에만 사용한다.)

FN은 오직 train에서 사용되고, inference(추론)에서는 사용되지 않는다.


### FAN training

ResNet + (BiLSTM) + AN + FN = FAN 이렇게 하나의 네트워크로 만든다.(Figure4 참고)

![Figure 4](/assets/images/post/attn/figure4.PNG){: .center}
- CNN-BiLSTM encoder : input image $$I$$를 high level sequece of feautures로 변환한다.
- RNN(LSTM) decoder : 각 target character를 생성한다.
- AN : extrated features를 이용하여, alignment factors와 glimpse vectors를 생성한다.
- FN : input image의 적절한 target character 영역에 AN의 attention을 집중시킨다.
- FN과 AN은 동시에 train한다.

전체 loss function(objective function)은 target-generation과 attention-focusing 둘다 고려하여 구성된다.

$$L = (1 - \lambda)L_{Att} + \lambda L_{focus}$$
- $$\lambda (0 <= \lamda < 1)$$ : tunable parameters, AN과 FN loss의 weight를 결정한다.

standard back-propagation에 의해 train된다.

#### decoding

- attention-based decoder : 함축적으로 학습한 character-label 확률 통계로부터 characters의 output sequence를 생성하는 것을 말한다.
- 제한이 없는 text recognition 과정에서(lexicon-free), 쉽게 가장 가능성있는 character를 선택한다.
- 반면 제한된 text recognition에서는(크기가 다른 어휘), 모든 어휘(lexicon words)에 대해 조건부 확률 분포를 계산하고, 가장 높은 확률의 것을 output 결과로 선택한다.
