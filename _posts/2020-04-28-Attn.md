---
title: Attention-based sequence prediction 이해하기
layout: single
author_profile: true
read_time: true
use_math: true
comments: true
share: true
related: true
categories:
- str
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: STR
article_tag2: scene text recognition
article_tag3: FAN
article_tag4: focusing attention network
last_modified_at: 2020-04-28 18:07:00 +0800
---

Attention-based sequence prediction에 관한 글입니다.

[참고 논문]
- [[FAN]](https://arxiv.org/abs/1709.02054) Focusing attention: Towards accurate text recognition in natural images
- [[RARE]](https://arxiv.org/abs/1603.03915) Robust Scene Text Recognition with Automatic Rectification

## RARE

TPS-VGG-BiLSTM-Attn

### SRN : Sequence Recognition Network

- attention-based model로, input image로부터 sequence를 인식한다.
- input : rectified image(STN output), 이상적으로 문자들이 왼쪽에서 오른쪽으로 수평적 이미지
- input으로부터 sequential representation을 extract하고, 단어를 인식한다.
- SRN은 encoder와 decoder를 가진다.
    - encoder : input image로부터 sequential representation을 extract한다.
    - decoder : sequential representation에 따른 sequence를 생성한다.

![Figure 5](/assets/images/post/attn/figure5_attn.PNG)

#### 1. Encoder : Convolutional-Recurrent Network

CNN으로부터 feature map을 추출하여, map-to-sequence 연산을 통해 feature map을 여러 연속된 조각으로 분리한다.

[Figure 5]
- several convolutional layers : (input image의 robust하고 high-level의 descriptions이 포함된) feature maps 생성한다.
    - feature maps : depth D * height H * width W
- map-to-sequence operation : map의 columns를 왼쪽에서 오른쪽으로 가져와 vectors로 flatten한다.
    - sequence of W vectors : DW dimensions
    - column : local image region(receptive field)에 대응하고, 해당 region에 대한 descriptor이다.
- BLSTM : 2 layer Bidirectional Long-Short Term Memory network
    - receptive field 크기에 의해 제한되므로, feature sequence는 limited image contexts에 영향을 준다.?
    - sequence 내의 long-term dependencies를 model하기 위해서 사용한다.
    - 양방향으로 sequence 내의 dependencies를 분석할 수 있는 recurrent network
    - output은 input과 동일한 길이(L = W)의 sequence


#### 2. Decoder : Recurrent Character Generator



## FAN : Focusing Attention Network

?-ResNet-?-Attn

**기존 attention-based method 문제**
- complicated, low-quality 이미지에서 성능이 떨어짐
- cannot get accurate alignments between feature areas and targets for such images
--> **"attention drift"**

### FAN method

FAN = AN + FN

![Figure 2](/assets/images/post/attn/figure2.png)

Figure2 (a)를 보면 attention이 제대로 이루어지지 않고 있다. 이러한 문제를 "attention drift"라고 한다. (b)와 같이 FN을 추가하여 문제점을 해결한다.

![Figure 5](/assets/images/post/attn/figure5_fan.PNG)

- alignment factors(target labels & features 사이) 생성된다. 각 alighment factor는 input image의 attention region과 대응한다.
- bad alignment(벗어나거나 unfocused attention region)는 poor recognition을 보인다.
- FN component는...
    - 1\) 각 target label에 대해 attention region 위치를 찾고,
    - 2\) 대응하는 glimpse vector와 함께 attention region으로부터 dense prediction을 한다.
- 이렇게 FN은 glimpse vector가 reasonable한지 판단할 수 있다.

summary
- FN은 AN에서 제공하는 glimpse vector를 바탕으로, input image의 attention region에 대해 dense output을 생성한다.
- AN은 FN의 feedback을 바탕으로, glimpse vector를 업데이트한다.

#### AN : Attention Network

recognizing character targets (기존 방법처럼)

1. attention based decoder
- RNN, inut image I로부터 target sequence를 직접 생성한다.
    - 실제로, image $$I$$는 CNN-LSTM에 의해 a sequence of feature vectors로 종종 encode된다.
    - $$I = Encoder(h_1, ... , h_T)$$

![Formula 1](/assets/images/post/attn/formula1.png)
- Generate() : feed-forward network
- RNN() : LSTM recurrent network

- EOS(end-of-sentence) token을 target set에 추가한다.
- decoder는 가변적인 길이의 sequence를 다루기 때문에 EOS가 나오면 문자 생성을 완료한다.

2. Loss Function

![Formula 2](/assets/images/post/attn/formula2.png)

3. drawbacks
    1) attention drift

    - complicated, low-quality 이미지에 영향을 많이 받는다.
    - 정확하지 않은 alignment factors를 생성한다.
        - integration of glimpse vectors에 대한 alignment constraint(제약)이 모델에 없기 때문이다.
        - 이로 인해, attention regions과 ground-truth regions가 불일치 할 수 있다.

    - FAN 논문에서는 이를 해결하는 것을 목표로 한다.
    - [[FN]](#fn-:-focusing-network)을 도입하여, 각 target character로 AN의 attention을 제한하고자 한다.

    2) huge scene text data(e.g. 8000만 개 synthetic data)에 대해서는 모델을 train하기 어렵다.



#### FN : Focusing Network

adjusting attention by evaluating whether AN pays attention properly on the target areas in the images

![Figure 3](/assets/images/post/attn/figure3.png)

#### FAN training

![Figure 4](/assets/images/post/attn/figure4.PNG)

#### decoding
