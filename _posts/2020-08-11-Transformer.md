---
title: Transformer (Attention is All You Need)
layout: single
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: Transformer
article_tag2: Attention
last_modified_at: 2020-08-11 17:57:00 +0800
---

[Attention Is All You Need](https://arxiv.org/abs/1706.03762) 논문 관련 정리한 글입니다.

참고

<https://www.theteams.kr/teams/2829/post/69500>

## Transformer

RNN은 long-term dependency()에 취약하며, CNN은 




참고

<https://www.theteams.kr/teams/2829/post/69500> <br>
<https://namhandong.tistory.com/48> <br>
<https://medium.com/@omicro03/attention-is-all-you-need-transformer-paper-%EC%A0%95%EB%A6%AC-83066192d9ab> <br>
<https://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221566232632&categoryNo=75&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView> <br>
