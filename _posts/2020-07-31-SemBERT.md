---
title: SemBERT 정리
layout: single
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: NLP
article_tag2: BERT
last_modified_at: 2020-07-31 12:03:00 +0800
---

[Semantics-aware BERT for Language Understanding](https://arxiv.org/abs/1909.02209) 논문을 정리한 글입니다.

## abstract

최근 language representation 연구에서는 contextualized features를 language model training에 통합하여 다양한 기계 독해 및 NLI에 좋은 결과를 보였다. 하지만 기존 language representation 모델(ELMo, GPT, BERT 등)은 문자 또는 word embedding과 같은 plain context-sensitive features만 활용한다. 이러한 모델은 language representation에 풍부한 의미(seamantic)를 제공할 수 있는 structured semantic 정보를 통합하는 것을 거의 고려하지 않는다.

NLU를 증진시키기 위해...

1. pre-trained semantic role labeling으로부터 explicit contextual semantics를 통합한다.
2. BERT backbone에 걸쳐 contextual semantics을 명시적으로 흡수할 수 있는 향상된 language representation 모델인 Semantics-aware BERT(SemBERT)를 소개한다.

SemBERT는 상당한 작업별 수정 없이 BERT 전구체의 편리한 사용성을 light fine-tuning 방법으로 유지한다. BERT에 비해 SemBERT는 개념상 단순하지만 더욱 강력하다. 10가지 독해와 NLI 과제에서 새로운 최첨단 기술을 습득하거나 성과를 크게 향상시킨다.


## introduction

현재의 NLU 모델은 불충분한 contextual semantic representation 및 learning으로 어려움을 겪고 있다. 실제로 NLU는 sentence contextual semantic analysis와 유사한 목적을 공유한다. 간단히 말해, 문장의 SRL(semantic role labeling)은 문장의 중심적 의미와 관련하여 *who did what to whon, when, why*를 발견하는 것으로 NLU의 과제 목표와 일치한다.

human language에서 문장은 보통 다양한 predicate-argument 구조를 포함하는 반면, neural model은 multiple semantic 구조를 모델링하는 것에 대해 거의 고려하지 않고 문장을 embedding representation으로 encoding 한다.

우리는 명시적인 contextual semantic 단서가 있는 fine-tuned BERT인 SemBERT를 제시함으로써 문장의 contextual semantics를 multiple predicate-specific argument 시퀀스에서 풍부하게 하고자 한다. 제안된 SemBERT는 fine-grained manner로 표현을 학습하며, 보다 깊은 의미 표현을 위한 plain context representation 그리고 explicit semantics에 대한 BERT의 장점을 모두 가진다.

우리 모델은 다음과 같은 세 가지 요소로 구성된다.

1. out-of-shelf semantic role labeler : 다양한 semantic role label로 입력 문장에 주석을 달기 위함.
2. sequence encoder : pre-trained language model을 사용하여 input raw text에 대한 표현을 작성하고 semantic role label이 병렬로 삽입되도록 매핑함.
3. semantic integration component : text representation component를 contextual explicit semantic embedding과 통합하여 downstream tasks에 대한 joint representation을 얻음.


## background & related work

### Language Modeling for NLU

NLU(Natural Language Understanding)은 자연어에 대한 포괄적인 이해와 추론 능력이 필요하다. 최근 NLU 연구에서는 stacked attention mechanism 또는 large-scale corpus로 인해 모델이 점점 더 정교해져 계산 비용이 폭발적으로 증가한다.

distributed representations은 large-scale unlabeled text에서 단어의 local co-occurence를 캡처할 수 있기 때문에 NLP 모델의 standard part로 널리 사용되어 왔다. 하지만 이러한 접근 방식은 sentence level에서 contextual encoding을 고려하여 각 단어에 대해 

본 논문에서는 context-sensitive features를 추출하는 방법을 따르고, explicit context semantics를 공동으로(jointly) 학습하기 위해 pre-trained BERT를 backbone encoder로 사용한다.

### explicit contextual semantics


#### formal semantic frames

- FrameNet
- PropBank : computational linguistics에서 보다 대중적으로 구현됨

formal semantics(형식적인 의미론)은 일반적으로 semantic 관계를 predicate-argument 구조로 제시한다.

예를 들어, target verb (predicate) ***"sold"*** 일 때 모든 argument는 label이 다음과 같이 지정된다.

![formal semantic](/assets/images/post/sembert/formal-semantic.PNG)

- $$ARG_0$$ : agent, 판매자
- $$ARG_1$$ : theme, 팔린 물건
- $$ARG_2$$ : recipient, 구매자
- $$AM-TMP$$ : adjunct indicating the timing of the action
- $$V$$ : predicate, 술어

#### Semantic Role Labeling (SRL)

predicate-argument 구조를 parse하기 위한 NLP task

최근 end-to-end SRL system neural model이 소개되었다. 이 연구는 argument identification과 argument classification을 one-shot으로 다룬다. 최근에는 SRL을 쉽게 NLU에 통합할 수 있다.

## Semantics-aware BERT

![SemBERT model](/assets/images/post/sembert/SemBERT.png)




### Semantic Role Labeling

![input representation](/assets/images/post/sembert/input-representation.PNG)

### Encoding


### Intergration


## model implementation


## experiments


## anlysis

![GLUE acc](/assets/images/post/sembert/glue-results.PNG)

![SNLI acc](/assets/images/post/sembert/snli-result.PNG)

---

**참고**

<https://jeonsworld.github.io/NLP/sembert/>
