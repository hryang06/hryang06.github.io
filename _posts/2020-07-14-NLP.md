---
title: NLP 정리
layout: single
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: NLP
article_tag2: natural language processing
last_modified_at: 2020-07-14 10:09:00 +0800
---

## NLP(Natural Language Processing) 자연어 처리

![NLP](/assets/images/post/nlp/nlp-nlu-nlg.png)

- NLU(Natural Language Understanding) 자연어 이해 : 자연어 형태의 문장을 이해하는 기술
    - 사람-기계 상호작용이 필수인 경우 NLU는 핵심 기술
    - ex) 구글에서 NLU 기술을 접목해 기존 키워드 매칭 방식과 비교해 더 나은 검색 서비스를 제공함. ([BERT](https://hryang06.github.io/nlp/BERT/))
- NLG(Natural Language Generation) 자연어 생성 : 자연어 문장을 생성하는 기술
- NLU와 NLG는 사람-기계, 기계-기계 사이 사람이 자연어로 소통하는 데 필요한 기술

![NLU](/assets/images/post/nlp/nlu-ex.png)


## NLU(Natural Language Understanding) 자연어 이해

### Language Model (LM)

![NLP](/assets/images/post/nlp/lm-1819.png)

#### 언어 모델 성능 평가

##### GLUE

[GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461) <br>
<https://gluebenchmark.com/>


- 사전 학습된 딥러닝 기반 언어 모델인 ELMo, GPT-1, BERT 모두 GLUE 벤치마크에서 당시 최고의 성능을 보임

- baseline : BiLSTM

- dataset (9)
    - single-sentence tasks
        - CoLA (Corpus of Linguistic Acceptability)
        - SST-2 (Stanford Sentiment Treebank)

    - similarity and paraphrase tasks
        - MRPC (Microsoft Research Paraphrase Corpus)
        - QQP (Quora Question Pairs)
        - STS-B (Semantic Textual Similarity Benchmark)

    - inference tasks
        - MNLI (Multi-Genre Natural Language Inference)
        - QNLI (Question Natural Language Inference)
        - RTE (Recognizing Textual Entailment)
        - WNLI (Winograd Natural Language Inference)

| corpus | task                | metrics                        | label                                | size (train/dev/test) |
|--------|---------------------|--------------------------------|--------------------------------------|-----------------------|
| CoLA   | acceptability       | Matthews corr.                 | acceptable / not acceptable          | 10K / 1K / 1.1K       |
| SST-2  | sentiment           | acc.                           | positive / negative                  | 67K / 872 / 1.8K      |
| MRPC   | paraphrase          | acc. / F1                      | same / not same                      | 1.7K / 408 / 3.6K     |
| QQP    | paraphrase          | acc. / F1                      | same / not same                      | 400K / - / 391K       |
| STS-B  | sentence similarity | Pearson/Spearman corr.         | 1 ~ 5 (similarity score)             | 7K / 1.5K / 1.4K      |
| MNLI   | NLI                 | metched acc. / mismatched acc. | entailment / contradiction / neutral | 393K / 20K / 20K      |
| QNLI   | QA/NLI              | acc.                           |                                      |                       |
| RTE    | NLI                 | acc.                           | entailment / not entailment          | 2.7K / - / 3K         |
| WNLI   | coreference/NLI     | acc.                           |                                      | 706 / - / 146         |


##### SuperGLUE

[SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537) <br>
<https://super.gluebenchmark.com/>

- GLUE보다 더 어려운 NLU 과제를 모은 벤치마크

- baseline : BERT

- dataset (8)
    - BoolQ (Boolean Questions)
    - CB (CommitmentBank)
    - COPA (Choice of Plausible Alternatives)
    - MultiRC (Multi-Sentence Reading Comprehension)
    - ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)
    - RTE (Recognizing Textual Entailment)
    - WiC (Word-in-Context)
    - WSC (Winograd Schema Challenge)

| corpus  | task   | metrics   | label                            | size (train/dev/test) | text sources                    |
|---------|--------|-----------|----------------------------------|-----------------------|---------------------------------|
| BoolQ   | QA     | acc.      | yes / no                         | 9427 3270 / 3245      | Google queries, Wikipedia       |
| CB      | NLI    | acc. / F1 | true / false / unknown           | 250 / 57 / 250        | various                         |
| COPA    | QA     | acc.      | 2 choices (correct cause/effect) | 400 / 100 / 500       | blogs, photography encyclopedia |
| MultiRC | QA     | F1a / EM  | true / false                     | 5100 / 953 / 1800     | various                         |
| ReCoRD  | QA     | F1 / EM   | 주어진 entities 리스트에서 예측    | 101K / 10K / 10K      | news (CNN, Daily Mail)          |
| RTE     | NLI    | acc.      | entailment / not entailment      | 2500 / 278 / 300      | news, Wikipedia                 |
| WiC     | WSD    | acc.      | true / false                     | 6000 / 638 / 1400     | WordNet, VerbNet, Wiktionary    |
| WSC     | coref. | acc.      | true / false                     | 554 / 104 / 146       | fiction books                   |

\* EM : exact match

## NLI(Natural Language Inference) 자연어 추론

[KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding](https://arxiv.org/abs/2004.03289) 정리한 글
: <https://hryang06.github.io/nlp/KorNLI-KorSTS/>


참조

<https://www.kakaobrain.com/blog/118> <br>
