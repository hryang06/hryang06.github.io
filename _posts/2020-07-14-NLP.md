---
title: NLP 이해하기
layout: single
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: NLP
article_tag2: natural language processing
last_modified_at: 2020-07-14 10:09:00 +0800
---

## NLP(Natural Language Processing) 자연어 처리

![NLP](/assets/images/post/nlp/nlp-nlu-nlg.png)

- NLU(Natural Language Understanding) 자연어 이해 : 자연어 형태의 문장을 이해하는 기술
    - 사람-기계 상호작용이 필수인 경우 NLU는 핵심 기술
    - ex) 구글에서 NLU 기술을 접목해 기존 키워드 매칭 방식과 비교해 더 나은 검색 서비스를 제공함. ([BERT 정리](https://hryang06.github.io/nlp/BERT/))
- NLG(Natural Language Generation) 자연어 생성 : 자연어 문장을 생성하는 기술
- NLU와 NLG는 사람-기계, 기계-기계 사이 사람이 자연어로 소통하는 데 필요한 기술

![NLU](/assets/images/post/nlp/nlu-ex.png)


## NLU(Natural Language Understanding) 자연어 이해

### Language Model (LM)

![NLP](/assets/images/post/nlp/lm-1819.png)

#### 언어 모델 성능 평가 (dataset)

##### GLUE

[GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461) <br>
<https://gluebenchmark.com/>


- 사전 학습된 딥러닝 기반 언어 모델인 ELMo, GPT-1, BERT 모두 GLUE 벤치마크에서 당시 최고의 성능을 보임

- baseline : BiLSTM

- dataset (9)
    - single-sentence tasks
        - CoLA (Corpus of Linguistic Acceptability)
        - SST-2 (Stanford Sentiment Treebank)

    - similarity and paraphrase tasks
        - MRPC (Microsoft Research Paraphrase Corpus)
        - QQP (Quora Question Pairs)
        - STS-B (Semantic Textual Similarity Benchmark)

    - inference tasks
        - MNLI (Multi-Genre Natural Language Inference)
        - QNLI (Question Natural Language Inference)
        - RTE (Recognizing Textual Entailment)
        - WNLI (Winograd Natural Language Inference)

| corpus | task                | metrics                        | label                                | size (train/dev/test) |
|--------|---------------------|--------------------------------|--------------------------------------|-----------------------|
| CoLA   | acceptability       | Matthews corr.                 | acceptable / not acceptable          | 10K / 1K / 1.1K       |
| SST-2  | sentiment           | acc.                           | positive / negative                  | 67K / 872 / 1.8K      |
| MRPC   | paraphrase          | acc. / F1                      | same / not same                      | 1.7K / 408 / 3.6K     |
| QQP    | paraphrase          | acc. / F1                      | same / not same                      | 400K / - / 391K       |
| STS-B  | sentence similarity | Pearson/Spearman corr.         | 1 ~ 5 (similarity score)             | 7K / 1.5K / 1.4K      |
| MNLI   | NLI                 | metched acc. / mismatched acc. | entailment / contradiction / neutral | 393K / 20K / 20K      |
| QNLI   | QA/NLI              | acc.                           |                                      | ? 105K / 5463 /       |
| RTE    | NLI                 | acc.                           | entailment / not entailment          | 2.7K / - / 3K         |
| WNLI   | coreference/NLI     | acc.                           |                                      | 706 / - / 146         |


##### SuperGLUE

[SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537) <br>
<https://super.gluebenchmark.com/>

- GLUE보다 더 어려운 NLU 과제를 모은 벤치마크

- baseline : BERT

- dataset (8)
    - BoolQ (Boolean Questions)
    - CB (CommitmentBank)
    - COPA (Choice of Plausible Alternatives)
    - MultiRC (Multi-Sentence Reading Comprehension)
    - ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)
    - RTE (Recognizing Textual Entailment)
    - WiC (Word-in-Context)
    - WSC (Winograd Schema Challenge)

| corpus  | task   | metrics   | label                            | size (train/dev/test) | text sources                    |
|---------|--------|-----------|----------------------------------|-----------------------|---------------------------------|
| BoolQ   | QA     | acc.      | yes / no                         | 9427 / 3270 / 3245    | Google queries, Wikipedia       |
| CB      | NLI    | acc. / F1 | true / false / unknown           | 250 / 57 / 250        | various                         |
| COPA    | QA     | acc.      | 2 choices (correct cause/effect) | 400 / 100 / 500       | blogs, photography encyclopedia |
| MultiRC | QA     | F1a / EM  | true / false                     | 5100 / 953 / 1800     | various                         |
| ReCoRD  | QA     | F1 / EM   | 주어진 entities 리스트에서 예측    | 101K / 10K / 10K      | news (CNN, Daily Mail)          |
| RTE     | NLI    | acc.      | entailment / not entailment      | 2500 / 278 / 300      | news, Wikipedia                 |
| WiC     | WSD    | acc.      | true / false                     | 6000 / 638 / 1400     | WordNet, VerbNet, Wiktionary    |
| WSC     | coref. | acc.      | true / false                     | 554 / 104 / 146       | fiction books                   |

\* EM : exact match

##### SQuAD (Stanford Question Answering Dataset)

<https://rajpurkar.github.io/SQuAD-explorer/>

Machine Reading Comprehension (기계 독해)

![SQuAD dataset](/assets/images/post/nlp/squad-2.0-1.1.PNG)

- [SQuAD 2.0](https://arxiv.org/abs/1806.03822)
    - SQuAD 1.1 + 새로운 5만개 이상의 unanswerable questions
    - unanswerable question은 온라인의 crowd worker(기계 아닌 진짜 사람)가 직접 생성하였다.
    - 사람이 직접 했기 때문에 answerable question과 유사하여 기계적으로 판별이 어렵다.
    - negative example : unanswerable question의 답
    ![SQuAD ex](/assets/images/post/nlp/squad-examples.PNG)

- [SQuAD 1.1](https://arxiv.org/abs/1606.05250)

## NLI(Natural Language Inference) 자연어 추론

- Korean NLI
    - [KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding](https://arxiv.org/abs/2004.03289)
    - 정리한 글 : <https://hryang06.github.io/nlp/KorNLI-KorSTS/>

### SNLI(Stanford Natural Language Interence) Corpus

<https://nlp.stanford.edu/projects/snli/>

| Text   | Judgments   | Hypothesis  |
|--------|-------------|-------------|
|A man inspects the uniform of a figure in some East Asian country. | contradiction<br>C C C C C | The man is sleeping. |
|An older and younger man smiling. | neutral<br>N N E N N | Two men are smiling and laughing at the cats playing on the floor. |
|A black race car starts up in front of a crowd of people. | contradiction<br>C C C C C | A man is driving down a lonely road. |
|A soccer game with multiple males playing. | entailment<br>E E E E E | Some men are playing a sport. |
|A smiling costumed woman is holding an umbrella. | neutral<br>N N E C N | A happy woman in a fairy costume holds an umbrella. |

- MNLI와 같은 포맷

- label
    - entailment : premise가 hypothesis를 포함하는지
    - contradiction : premise가 hypothesis와 모순되는지
    - neutral : 그 외

- state-of-the-art : [SemBERT](https://arxiv.org/abs/1909.02209) (91.9%) (20.07.14 
기준)


### MNLI (Multi-Genre Natural Language Inference)


### QNLI (Question Natural Language Inference)


### RTE (Recognizing Textual Entailment)


## SRL(Semantic Role Labeling) 의미역 결정

<https://cs.kangwon.ac.kr/~leeck/NLP/SRL.pdf>


---

참조

<https://www.kakaobrain.com/blog/118> <br>
