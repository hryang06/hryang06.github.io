---
title: BERT 정리
layout: single
classes: wide
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: NLP
article_tag2: BERT
last_modified_at: 2020-07-15 17:03:00 +0800
---

# BERT : Bidirectional Encoder Representations from Transformers

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 논문을 참고하였습니다.

- 18년 10월 공개한 구글의 새로운 language representation model
- NLP 11개의 task에서 최고 성능을 보임

**2 model size for BERT**
- BERT-BASE
- BERT-LARGE



## model

![Transformer](/assets/images/post/bert/transformer.PNG){: .center}

BERT의 모델은 Transformer 기반으로 하며, BERT는 그림에서 왼쪽 encoder만 사용하는 모델이다.

[Transformer 참고](https://hryang06.github.io/nlp/Transformer/)



### Input Embedding

![Figure 2](/assets/images/post/bert/figure-2.PNG)

아래 세가지 embedding을 합하여 하나의 embedding으로 만든 후, layer normalization 그리고 dropout을 적용하여 input으로 사용한다.

#### 1. Token Embeddings
30,000 token vacabulary와 함께 WordPiece embeddings를 사용하였다. 흔히 사용하는 word embedding 방식을 사용하지 않았다. 모든 sequence의 첫번째 token은 항상 [CLS]이다. 두 문장은 [SEP]를 통해 구분한다.

#### 2. Segment Embeddings
[SEP]를 기준으로 두 문장을 구분한다.

#### 3. Position Embeddings
self-attention은 입력 위치는 고려하지 못하였다. 위치 정보를 넣어 주어 준다.


### Encoder Block


---

![Figure 1](/assets/images/post/bert/figure-1.PNG)

## Pre-Training

- 사전학습
- unsupervised learning
- general-purpose language understanding 모델 구축

기존의 left-to-right 또는 right-to-left language model을 사용하지 않았다.
두가지 unsupervised tasks를 사용하여 pre-train 하였다.

### task #1 : Masked LM (MLM)

![mlm](/assets/images/post/bert/bert-mlm.png)

입력에서 token 하나를 가리고(mask) 해당 token을 맞추는 Language Model(LM)

모든 WordPiece tokens의 15%를 [MASK] token으로 바꾼다.
- 80% of time : [MASK] token 변경
- 10% of time : random word 변경
- 10% of time : 변경 x

![mlm-example](/assets/images/post/bert/bert-mlm-ex.PNG)

### task #2 : Next Sentence Prediction (NSP)

![mlm](/assets/images/post/bert/bert-nsp.png)

두 문장에 대해 두번째 문장이 첫번째 문장 바로 다음에 오는 문장인지 예측하여 문맥과 순서를 학습할 수 있다. 두 문장은 [SEP] token으로 구분한다.

    input = [CLS] the man went to [MASK] store [SEP]
            he bought a gallon [MASK] milk [SEP]
    output = IsNext
    
    input = [CLS] the man [MASK] to the strore [SEP]
            penguin [MASK] are flight ##less birds [SEP]
    output = NotNext


## Fine-Tuning

- 미세조정
- supervised learning
- downstream NLP task(QA, STS 등)에 적용하는 semi-supervised learning 모델

위에서 pre-train한 parameter로 초기화하여 parameter를 fine-tuning 한다. 

![Figure 4](/assets/images/post/bert/figure-4.PNG)


# RoBERTa : Robustly optimized BERT Approach

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) 논문을 참고하였습니다.

BERT와 다른 점

- large-scale text copora dataset
- dynamic masking
- model input format and next sentence

## model

![RoBERTa Dev Set](/assets/images/post/bert/roberta-dev-set.PNG)

### additional data [total 160GB]

1. BookCorpus + (english) Wikipedia [16GB]
- original data used to train BERT

2. CC-News [+76GB]
- collected from English portion of the CommonCrawl News dataset
- contain 63million English news articles (2016.09 ~ 2019.02)
- 76GB after filtering

3. OpenWebText [+38GB]
- open-source recreation of the WebText corpus described in Radford et al.
- text is web content extracted from URLs shared on Reddit with at least three upvotes

4. Stories [+31GB]
- introduced in Trinh and Le containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemes

### pretrain longer

#### Static vs. Dynamic Masking


#### Model Input Format & Next Sequence Prediction

#### training with Large Batches

#### Text Encoding

# ALBERT : A Lite BERT

[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) 논문을 참고하였습니다.


## model

![ALBERT config](/assets/images/post/bert/bert-albert-config.PNG)

---

참조

- BERT

<http://jalammar.github.io/illustrated-bert/> <br>
<https://keep-steady.tistory.com/19> <br>
<http://docs.likejazz.com/bert/>

- RoBERTa

<https://jeonsworld.github.io/NLP/roberta/>

- ALBERT
