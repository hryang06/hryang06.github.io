---
title: BERT 정리
layout: single
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- nlp
toc: true
toc_sticky: true
toc_label: 목차
article_tag1: NLP
article_tag2: BERT
last_modified_at: 2020-07-15 17:03:00 +0800
---

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 논문을 참고하였습니다.

## BERT : Bidirectional Encoder Representations from Transformers

- 18년 10월 공개한 구글의 새로운 language representation model
- NLP 11개의 task에서 최고 성능을 보임

**2 model size for BERT**
- BERT-BASE
- BERT-LARGE

---

### input/output representations

![Figure 2](/assets/images/post/bert/figure-2.PNG)

아래 세가지 embedding을 합하여 하나의 embedding으로 만든 후, layer normalization 그리고 dropout을 적용하여 input으로 사용한다.

#### 1. Token Embeddings
30,000 token vacabulary와 함께 WordPiece embeddings를 사용하였다. 흔히 사용하는 word embedding 방식을 사용하지 않았다. 모든 sequence의 첫번째 token은 항상 [CLS]이다. 두 문장은 [SEP]를 통해 구분한다.

#### 2. Segment Embeddings
[SEP]를 기준으로 두 문장을 구분한다.

#### 3. Position Embeddings
self-attention은 입력 위치는 고려하지 못하였다. 위치 정보를 넣어 주어 준다.

---

![Figure 1](/assets/images/post/bert/figure-1.PNG)

### pre-training

사전학습.

기존의 left-to-right 또는 right-to-left language model을 사용하지 않았다.
두가지 unsupervised tasks를 사용하여 pre-train 하였다.

#### task #1 : Masked LM (MLM)

![mlm](/assets/images/post/bert/bert-mlm.png)

입력에서 token 하나를 가리고(mask) 해당 token을 맞추는 Language Model(LM)

모든 WordPiece tokens의 15%를 [MASK] token으로 바꾼다.
- 80% of time : [MASK] token 변경
- 10% of time : random word 변경
- 10% of time : 변경 x

![mlm-example](/assets/images/post/bert/bert-mlm-ex.png)

#### task #2 : Next Sentence Prediction (NSP)

![mlm](/assets/images/post/bert/bert-nsp.png)

두 문장에 대해 두번째 문장이 첫번째 문장 바로 다음에 오는 문장인지 예측하여 문맥과 순서를 학습할 수 있다. 두 문장은 [SEP] token으로 구분한다.

    input = [CLS] the man went to [MASK] store [SEP] <br>
            he bought a gallon [MASK] milk [SEP] <br>
    output = IsNext
    
    input = [CLS] the man [MASK] to the strore [SEP] <br>
            penguin [MASK] are flight ##less birds [SEP] <br>
    output = NotNext


### fine-tuning

미세조정.

위에서 pre-train한 parameter로 초기화하여 parameter를 fine-tuning 한다. 실제로 

![Figure 4](/assets/images/post/bert/figure-4.PNG)


참조

<http://jalammar.github.io/illustrated-bert/>

<https://keep-steady.tistory.com/19>
